{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUjLb6EvwMie"
   },
   "source": [
    "# 8a â€“ Model Monitoring (Normal Conditions)\n",
    "\n",
    "This notebook deploys the selected production model\n",
    "and configures SageMaker Model Monitor to track\n",
    "data quality and input feature drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KBhrJSawMqa"
   },
   "source": [
    "## Import Required Libraries and Initialize AWS Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.joblib\n",
      "inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf model.tar.gz model.joblib inference.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to: s3://sagemaker-us-east-1-083422367993/ghcn-extreme/models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "model_s3_path = sagemaker_session.upload_data(\n",
    "    path=\"model.tar.gz\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=\"ghcn-extreme/models\"\n",
    ")\n",
    "\n",
    "print(\"Uploaded to:\", model_s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint deleted.\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "HdcNUIO4wLVj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-us-east-1-083422367993\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor\n",
    ")\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "\n",
    "print(\"Bucket:\", bucket)\n",
    "print(\"Region:\", region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: s3://sagemaker-us-east-1-083422367993/ghcn-extreme/models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "project_prefix = \"ghcn-extreme\"\n",
    "model_s3_path = f\"s3://{bucket}/ghcn-extreme/models/model.tar.gz\"\n",
    "\n",
    "print(\"Model path:\", model_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4l8Dpp5wVTH"
   },
   "source": [
    "## Deploy Model Endpoint\n",
    "\n",
    "The Random Forest model is selected as the production model\n",
    "for monitoring demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "OpBnAfrLwW8i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Fresh endpoint deployed.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "model_s3_path = f\"s3://{bucket}/ghcn-extreme/models/model.tar.gz\"\n",
    "\n",
    "sk_model = SKLearnModel(\n",
    "    model_data=model_s3_path,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "predictor = sk_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "print(\"Fresh endpoint deployed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO4wunaKwfTY"
   },
   "source": [
    "## Configure Model Monitor (Data Quality Monitoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "hUujaHJ1whdw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-02-16-04-28-35-989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[34m2026-02-16 04:32:17.945125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:17.945170: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:19.681499: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:19.681540: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:19.681615: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-79-199.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:19.681948: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,380 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:083422367993:processing-job/baseline-suggestion-job-2026-02-16-04-28-35-989', 'ProcessingJobName': 'baseline-suggestion-job-2026-02-16-04-28-35-989', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-083422367993/ghcn-extreme/processed_csv/', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-083422367993/ghcn-extreme/monitoring/baseline', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::083422367993:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,380 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,380 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,380 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,380 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,381 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,627 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,628 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,628 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,640 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,640 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:21,641 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:23,178 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.79.199\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/ha\u001b[0m\n",
      "\u001b[34mdoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:23,204 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:23,210 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-aa886c20-6cef-405b-8bf5-806378628607\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,094 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,116 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,117 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,121 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,136 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,136 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,136 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,137 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,201 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,222 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,222 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,226 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,230 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 16 04:32:24\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,232 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,232 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,235 INFO util.GSet: 2.0% max memory 1.4 GB = 28.4 MB\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,235 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,269 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,274 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,274 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,274 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,275 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,276 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,276 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,341 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,341 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,342 INFO util.GSet: 1.0% max memory 1.4 GB = 14.2 MB\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,342 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,343 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,343 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,343 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,343 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,349 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,354 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,354 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,354 INFO util.GSet: 0.25% max memory 1.4 GB = 3.5 MB\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,354 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,363 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,364 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,364 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,368 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,368 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,371 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,371 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,371 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 435.9 KB\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,371 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,404 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1095380988-10.0.79.199-1771216344394\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,426 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,443 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,564 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,586 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,596 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.79.199\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:24,616 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:26,707 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:26,707 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:28,905 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:28,906 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:31,367 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:31,367 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:34,055 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:34,056 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:36,707 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:36,708 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:46,719 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:49,732 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:50,659 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:50,728 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:50,776 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,689 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,732 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,733 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,733 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,734 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,782 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5673, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,799 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,802 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,903 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,903 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,904 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,904 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:51,904 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,531 INFO util.Utils: Successfully started service 'sparkDriver' on port 42685.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,585 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,660 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,696 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,697 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,763 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,823 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-80b95c6b-760a-4674-a58c-e2cb145daee8\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,853 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,925 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:52,980 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.79.199:42685/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1771216371682\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:54,175 INFO client.RMProxy: Connecting to ResourceManager at /10.0.79.199:8032\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,502 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,503 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,513 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7736 MB per container)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,514 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,514 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,515 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,523 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:55,698 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:32:58,063 INFO yarn.Client: Uploading resource file:/tmp/spark-00d28d20-0fbb-4b33-aa08-46e249a7f5bb/__spark_libs__7152993417806733794.zip -> hdfs://10.0.79.199/user/root/.sparkStaging/application_1771216354492_0001/__spark_libs__7152993417806733794.zip\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,394 INFO yarn.Client: Uploading resource file:/tmp/spark-00d28d20-0fbb-4b33-aa08-46e249a7f5bb/__spark_conf__3829487411640003812.zip -> hdfs://10.0.79.199/user/root/.sparkStaging/application_1771216354492_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,458 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,458 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,460 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,461 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,462 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,509 INFO yarn.Client: Submitting application application_1771216354492_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:00,775 INFO impl.YarnClientImpl: Submitted application application_1771216354492_0001\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:01,782 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:01,786 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Feb 16 04:33:01 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771216380632\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771216354492_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:02,790 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:03,794 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:04,806 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:05,811 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:06,816 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:07,822 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:08,828 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:09,832 INFO yarn.Client: Application report for application_1771216354492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,339 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1771216354492_0001), /proxy/application_1771216354492_0001\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,838 INFO yarn.Client: Application report for application_1771216354492_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,839 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.79.199\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771216380632\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771216354492_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,841 INFO cluster.YarnClientSchedulerBackend: Application application_1771216354492_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,887 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46019.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,887 INFO netty.NettyBlockTransferService: Server created on 10.0.79.199:46019\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,890 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,899 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.79.199, 46019, None)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,908 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.79.199:46019 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.79.199, 46019, None)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,916 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.79.199, 46019, None)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:10,923 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.79.199, 46019, None)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:11,255 INFO util.log: Logging initialized @24043ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:13,292 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:19,276 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.79.199:45386) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:19,788 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:39445 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 39445, None)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:23,837 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:24,633 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:24,930 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:24,950 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:27,001 INFO datasources.InMemoryFileIndex: It took 92 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:27,311 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:27,855 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:27,861 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.79.199:46019 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:27,880 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,566 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,569 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,575 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 2149489\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,667 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,695 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,695 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,696 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,699 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,730 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,876 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,882 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,886 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.79.199:46019 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,887 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,920 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:28,923 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:29,070 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4637 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:29,663 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:39445 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:30,850 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:39445 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,466 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2429 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,473 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.671 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,469 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,485 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,638 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:31,645 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.977485 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:34,711 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:34,713 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:34,717 INFO datasources.FileSourceStrategy: Output Data Schema: struct<station_id: string, date: string, year: string, month: string, TMAX: string ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,009 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,041 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,043 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.79.199:46019 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,045 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,068 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,154 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,156 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,157 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,157 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,200 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,203 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,309 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,323 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,325 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.79.199:46019 (size: 7.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,329 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,333 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,333 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,342 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:35,512 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:39445 (size: 7.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:36,980 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:39445 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:37,849 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:39445 (size: 984.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,049 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2712 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,050 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.842 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,051 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,051 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,051 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,054 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.899589 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:38,407 INFO codegen.CodeGenerator: Code generated in 283.929516 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,308 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,760 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,768 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,774 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,775 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,785 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,798 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,857 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,861 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,862 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.79.199:46019 (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,863 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,867 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,867 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,893 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:39,965 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:39445 (size: 34.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,804 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2912 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,807 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.999 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,812 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,817 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,818 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,819 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,821 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,939 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,942 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,943 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,943 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,944 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,945 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,964 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,966 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,967 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.79.199:46019 (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,968 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,968 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,968 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:42,981 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,037 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:39445 (size: 45.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,181 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,857 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 877 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,861 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.907 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,864 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,865 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,866 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,877 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.937722 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:43,973 INFO codegen.CodeGenerator: Code generated in 63.667581 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,391 INFO codegen.CodeGenerator: Code generated in 70.461822 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,626 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,629 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,630 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,630 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,635 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,636 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,690 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,704 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,707 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.79.199:46019 (size: 16.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,708 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,710 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,710 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,716 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:44,742 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:39445 (size: 16.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,913 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 3197 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,923 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 3.283 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,924 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,925 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,925 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:47,928 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 3.302335 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,948 INFO codegen.CodeGenerator: Code generated in 148.618475 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,964 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,965 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,965 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,965 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,967 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,968 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,976 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 52.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,978 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,979 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.79.199:46019 (size: 18.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,980 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,981 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,981 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:48,983 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,002 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:39445 (size: 18.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,308 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 326 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,309 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.339 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,310 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,311 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,311 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,311 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,311 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,624 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.79.199:46019 in memory (size: 34.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,650 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:39445 in memory (size: 34.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,701 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.79.199:46019 in memory (size: 7.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,703 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:39445 in memory (size: 7.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,771 INFO codegen.CodeGenerator: Code generated in 302.32253 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,777 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:39445 in memory (size: 16.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,779 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.79.199:46019 in memory (size: 16.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,819 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,820 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,821 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,821 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,821 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,823 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,827 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 44.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,829 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,831 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.79.199:46019 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,831 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,832 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,843 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,846 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,869 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.79.199:46019 in memory (size: 18.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,880 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:39445 in memory (size: 18.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,881 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:39445 (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,930 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,947 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.79.199:46019 in memory (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:49,950 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:39445 in memory (size: 45.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,018 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 173 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,019 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.195 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,020 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,020 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,020 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,021 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.202401 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,158 INFO codegen.CodeGenerator: Code generated in 100.386876 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,320 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,327 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,328 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,329 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,329 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,329 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,333 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,346 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,348 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,349 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.79.199:46019 (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,350 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,351 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,351 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:50,393 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:39445 (size: 13.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,070 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 717 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,070 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,071 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 0.737 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,072 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,072 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,072 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,072 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,073 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,081 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,084 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,087 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.79.199:46019 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,087 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,088 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,088 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,113 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:39445 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,130 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,283 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 193 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,289 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.215 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,290 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,290 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,290 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,290 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 0.969211 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,869 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,870 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,870 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,870 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,871 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,874 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,890 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 72.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,894 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,895 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.79.199:46019 (size: 25.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,896 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,897 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,898 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,900 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:51,920 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:39445 (size: 25.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,704 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 804 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,706 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,707 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.827 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,709 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,710 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,710 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,710 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,759 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,761 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,762 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,762 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,763 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,764 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,773 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 140.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,776 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,778 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.79.199:46019 (size: 40.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,779 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,779 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,779 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,790 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,810 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:39445 (size: 40.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:52,830 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,020 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 231 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,020 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,021 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.254 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,022 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,022 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,026 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.265744 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,058 INFO codegen.CodeGenerator: Code generated in 28.639879 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,288 INFO codegen.CodeGenerator: Code generated in 36.615643 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,341 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,342 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,343 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,343 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,344 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,346 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,357 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,360 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,365 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.79.199:46019 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,366 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,366 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,368 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,370 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:53,384 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:39445 (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,310 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 940 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,310 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,311 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.962 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,312 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,313 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,313 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.972334 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,599 INFO codegen.CodeGenerator: Code generated in 72.82919 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,610 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,611 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,611 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,612 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,612 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,613 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,619 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 62.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,626 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,627 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.79.199:46019 (size: 21.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,628 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,629 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,629 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,631 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:54,645 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:39445 (size: 21.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,093 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 463 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,093 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,094 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.478 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,095 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,095 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,095 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,096 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,239 INFO codegen.CodeGenerator: Code generated in 70.505841 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,252 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,254 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,255 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,255 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,255 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,256 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,258 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 55.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,261 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,262 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.79.199:46019 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,263 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,263 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,263 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,265 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,318 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:39445 (size: 16.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,324 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,463 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 198 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,463 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,464 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.207 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,465 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,466 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,467 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.213753 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,651 INFO codegen.CodeGenerator: Code generated in 160.625952 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,808 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,809 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,810 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,810 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,811 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,811 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,818 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,828 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,837 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,842 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.79.199:46019 (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,843 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,844 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,845 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,847 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:55,890 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:39445 (size: 13.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,272 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 425 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,274 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,276 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.457 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,277 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,277 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,277 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,278 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,278 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,282 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,287 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,291 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.79.199:46019 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,293 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,303 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,303 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,305 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,321 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:39445 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,330 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,386 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,386 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,387 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.108 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,388 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,389 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,389 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.581297 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,538 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.79.199:46019 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,558 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:39445 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,615 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.79.199:46019 in memory (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,623 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:39445 in memory (size: 13.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,720 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:39445 in memory (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,727 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.79.199:46019 in memory (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,741 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.79.199:46019 in memory (size: 40.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,742 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:39445 in memory (size: 40.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,759 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:39445 in memory (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,762 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.79.199:46019 in memory (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,795 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:39445 in memory (size: 25.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,811 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.79.199:46019 in memory (size: 25.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,819 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,854 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.79.199:46019 in memory (size: 21.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,857 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:39445 in memory (size: 21.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,872 INFO codegen.CodeGenerator: Code generated in 17.947397 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,887 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.79.199:46019 in memory (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,889 INFO scheduler.DAGScheduler: Registering RDD 86 (count at StatsGenerator.scala:66) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,889 INFO scheduler.DAGScheduler: Got map stage job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,890 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,890 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,891 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,891 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,898 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 21.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,901 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,903 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.79.199:46019 (size: 10.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,904 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:39445 in memory (size: 16.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,905 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,907 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,907 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,909 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,923 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:39445 (size: 10.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,947 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.79.199:46019 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:56,953 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:39445 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,029 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.79.199:46019 in memory (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,033 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:39445 in memory (size: 13.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,058 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,059 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,060 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at StatsGenerator.scala:66) finished in 0.167 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,061 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,061 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,062 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,062 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,133 INFO codegen.CodeGenerator: Code generated in 37.38697 ms\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,151 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,152 INFO scheduler.DAGScheduler: Got job 15 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,154 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,154 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,155 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,155 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,160 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,165 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,166 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.79.199:46019 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,170 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,170 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,171 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,172 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,189 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:39445 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,202 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.79.199:45386\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,229 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,229 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,230 INFO scheduler.DAGScheduler: ResultStage 22 (count at StatsGenerator.scala:66) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,232 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,233 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,234 INFO scheduler.DAGScheduler: Job 15 finished: count at StatsGenerator.scala:66, took 0.082585 s\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,872 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,890 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,927 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,930 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,941 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:57,979 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,031 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,034 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,051 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,059 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,105 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,105 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,105 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,131 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,133 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-fec6fc3c-d7eb-4df1-b578-9ea80482870f\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,147 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-00d28d20-0fbb-4b33-aa08-46e249a7f5bb\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,237 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-02-16 04:33:58,237 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "Baseline statistics generated.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DatasetFormat\n",
    "\n",
    "baseline_job = monitor.suggest_baseline(\n",
    "    baseline_dataset=f\"s3://{bucket}/ghcn-extreme/processed_csv/\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f\"s3://{bucket}/{project_prefix}/monitoring/baseline\",\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"Baseline statistics generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TNb87hYwkJo"
   },
   "source": [
    "## Schedule Monitoring Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session(region_name=\"us-east-1\")\n",
    "sm_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sm_session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached to: sagemaker-scikit-learn-2026-02-16-04-10-18-956\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"sagemaker-scikit-learn-2026-02-16-04-10-18-956\"\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "\n",
    "print(\"Attached to:\", predictor.endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: ghcn-extreme-monitor-schedule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule created.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import BatchTransformInput\n",
    "\n",
    "batch_input = BatchTransformInput(\n",
    "    data_captured_destination_s3_uri=f\"s3://{bucket}/{project_prefix}/monitoring/batch_input\",\n",
    "    destination=\"/opt/ml/processing/input\",\n",
    "    dataset_format={\n",
    "        \"Csv\": {\n",
    "            \"Header\": False\n",
    "        }\n",
    "    },\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"ghcn-extreme-monitor-schedule\",\n",
    "    batch_transform_input=batch_input,\n",
    "    output_s3_uri=f\"s3://{bucket}/{project_prefix}/monitoring/reports\",\n",
    "    schedule_cron_expression=\"cron(0 * ? * * *)\"\n",
    ")\n",
    "\n",
    "print(\"Monitoring schedule created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYDgCuNnwpCR"
   },
   "source": [
    "## Confirm Monitoring Schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "nfkEY3luwr3Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:083422367993:monitoring-schedule/ghcn-extreme-monitor-schedule',\n",
       " 'MonitoringScheduleName': 'ghcn-extreme-monitor-schedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'DataQuality',\n",
       " 'CreationTime': datetime.datetime(2026, 2, 16, 4, 55, 0, 181000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2026, 2, 16, 4, 55, 3, 360000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'data-quality-job-definition-2026-02-16-04-54-59-636',\n",
       "  'MonitoringType': 'DataQuality'},\n",
       " 'ResponseMetadata': {'RequestId': 'ecb5f59d-b0d9-4e66-aa9c-cb458915de8c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ecb5f59d-b0d9-4e66-aa9c-cb458915de8c',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '516',\n",
       "   'date': 'Mon, 16 Feb 2026 04:55:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.describe_schedule()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk1V7vOswtBB"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The model endpoint is deployed with data capture enabled.\n",
    "A baseline has been generated and a monitoring schedule created.\n",
    "\n",
    "Model Monitor will now detect data quality deviations\n",
    "and feature distribution shifts.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
